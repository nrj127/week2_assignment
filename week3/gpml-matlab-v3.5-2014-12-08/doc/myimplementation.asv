write_fig = 0;
disp(' ')

disp('n1 = 80; n2 = 40;                   % number of data points from each class')
n1 = 80; n2 = 40;
disp('S1 = eye(2); S2 = [1 0.95; 0.95 1];           % the two covariance matrices')
S1 = eye(2); S2 = [1 0.95; 0.95 1];
disp('m1 = [0.75; 0]; m2 = [-0.75; 0];                            % the two means')
m1 = [0.75; 0]; m2 = [-0.75; 0];
disp(' ')

disp('x1 = bsxfun(@plus, chol(S1)''*gpml_randn(0.2, 2, n1), m1);')
x1 = bsxfun(@plus, chol(S1)'*gpml_randn(0.2, 2, n1), m1);
disp('x2 = bsxfun(@plus, chol(S2)''*gpml_randn(0.3, 2, n2), m2);')         
x2 = bsxfun(@plus, chol(S2)'*gpml_randn(0.3, 2, n2), m2);         
disp(' ')



disp('x = [x1 x2]''; y = [-ones(1,n1) ones(1,n2)]'';')
x = [x1 x2]'; y = [-ones(1,n1) ones(1,n2)]';
figure(6)
disp('plot(x1(1,:), x1(2,:), ''b+''); hold on;');
plot(x1(1,:), x1(2,:), 'b+', 'MarkerSize', 12); hold on
disp('plot(x2(1,:), x2(2,:), ''r+'');');
plot(x2(1,:), x2(2,:), 'r+', 'MarkerSize', 12);
disp(' ')

disp('[t1 t2] = meshgrid(-4:0.1:4,-4:0.1:4);')
[t1 t2] = meshgrid(-4:0.1:4,-4:0.1:4);
disp('t = [t1(:) t2(:)]; n = length(t);               % these are the test inputs')
t = [t1(:) t2(:)]; n = length(t);               % these are the test inputs
disp('tmm = bsxfun(@minus, t, m1'');')
tmm = bsxfun(@minus, t, m1');
disp('p1 = n1*exp(-sum(tmm*inv(S1).*tmm/2,2))/sqrt(det(S1));')
p1 = n1*exp(-sum(tmm*inv(S1).*tmm/2,2))/sqrt(det(S1));
disp('tmm = bsxfun(@minus, t, m2'');')
tmm = bsxfun(@minus, t, m2');
disp('p2 = n2*exp(-sum(tmm*inv(S2).*tmm/2,2))/sqrt(det(S2));')
p2 = n2*exp(-sum(tmm*inv(S2).*tmm/2,2))/sqrt(det(S2));
set(gca, 'FontSize', 24)
disp('contour(t1, t2, reshape(p2./(p1+p2), size(t1)), [0.1:0.1:0.9])')
contour(t1, t2, reshape(p2./(p1+p2), size(t1)), [0.1:0.1:0.9])
[c h] = contour(t1, t2, reshape(p2./(p1+p2), size(t1)), [0.5 0.5]);
set(h, 'LineWidth', 2)
colorbar
grid
axis([-4 4 -4 4])
if write_fig, print -depsc f6.eps; end
disp(' '); disp('Hit any key to continue...'); pause

disp(' ')
disp('meanfunc = @meanConst; hyp.mean = 0;')
meanfunc = @meanConst; hyp.mean = 0;
disp('covfunc = @covSEard;   hyp.cov = log([1 1 1]);')
covfunc = @covSEard;   hyp.cov = log([1 1 1]); % covariance changes
%covfunc = @covLIN;   hyp.cov = []; %
disp('likfunc = @likErf;')
likfunc = @likErf; % likelihood changes
%likfunc = @likLogistic
disp(' ')

disp('hyp = minimize(hyp, @gp, -40, @infEP, meanfunc, covfunc, likfunc, x, y);')
hyp = minimize(hyp, @gp, -40, @infEP, meanfunc, covfunc, likfunc, x, y);
disp('[a b c d lp] = gp(hyp, @infEP, meanfunc, covfunc, likfunc, x, y, t, ones(n, 1));')
%[a b c d lp] = gp(hyp, @infLaplace, meanfunc, covfunc, likfunc, x, y, t, ones(n,1)); % inference changes
[a b c d lp] = gp(hyp, @infKL
, meanfunc, covfunc, likfunc, x, y, t, ones(n,1));
disp(' ') 
figure(7)
set(gca, 'FontSize', 24)
disp('plot(x1(1,:), x1(2,:), ''b+''); hold on')
plot(x1(1,:), x1(2,:), 'b+', 'MarkerSize', 12); hold on
disp('plot(x2(1,:), x2(2,:), ''r+'')')
plot(x2(1,:), x2(2,:), 'r+', 'MarkerSize', 12)
disp('contour(t1, t2, reshape(exp(lp), size(t1)), 0.1:0.1:0.9)')
contour(t1, t2, reshape(exp(lp), size(t1)), 0.1:0.1:0.9)
[c h] = contour(t1, t2, reshape(exp(lp), size(t1)), [0.5 0.5]);
set(h, 'LineWidth', 2)
colorbar
grid
axis([-4 4 -4 4])
if write_fig, print -depsc f7.eps; end
disp(' '); disp('Hit any key to continue...'); pause
% % % 
% % % % disp('large scale classification using the FITC approximation')
% % % % disp('[u1,u2] = meshgrid(linspace(-2,2,5)); u = [u1(:),u2(:)];')
% % % % [u1,u2] = meshgrid(linspace(-2,2,5)); u = [u1(:),u2(:)]; clear u1; clear u2
% % % % disp('nu = size(u,1);')
% % % % nu = size(u,1);
% % % % disp('covfuncF = {@covFITC, {covfunc}, u};')
% % % % covfuncF = {@covFITC, {covfunc}, u};
% % % % disp('inffunc = @infFITC_Laplace;')
% % % % inffunc = @infFITC_EP;                     % one could also use @infFITC_Laplace
% % % % disp('hyp = minimize(hyp, @gp, -40, inffunc, meanfunc, covfuncF, likfunc, x, y);')
% % % % hyp = minimize(hyp, @gp, -40, inffunc, meanfunc, covfuncF, likfunc, x, y);
% % % % disp('[a b c d lp] = gp(hyp, inffunc, meanfunc, covfuncF, likfunc, x, y, t, ones(n,1));')
% % % % [a b c d lp] = gp(hyp, inffunc, meanfunc, covfuncF, likfunc, x, y, t, ones(n,1));
% % % % disp(' ')
% % % % figure(8)
% % % % set(gca, 'FontSize', 24)
% % % % disp('plot(x1(1,:), x1(2,:), ''b+''); hold on')
% % % % plot(x1(1,:), x1(2,:), 'b+', 'MarkerSize', 12); hold on
% % % % disp('plot(x2(1,:), x2(2,:), ''r+'')')
% % % % plot(x2(1,:), x2(2,:), 'r+', 'MarkerSize', 12)
% % % % disp('contour(t1, t2, reshape(exp(lp), size(t1)), 0.1:0.1:0.9)')
% % % % contour(t1, t2, reshape(exp(lp), size(t1)), 0.1:0.1:0.9)
% % % % [c h] = contour(t1, t2, reshape(exp(lp), size(t1)), [0.5 0.5]);
% % % % set(h, 'LineWidth', 2)
% % % % disp('plot(u(:,1),u(:,2),''ko'', ''MarkerSize'', 12)')
% % % % plot(u(:,1),u(:,2),'ko', 'MarkerSize', 12)
% % % % colorbar
% % % % grid
% % % % axis([-4 4 -4 4])

% test points 
test_points_output_1 = 100; test_points_output_2 = 120;

test_points_input_1 = bsxfun(@plus, chol(S1)'*gpml_randn(0.2, 2, test_points_output_1), m1);       
test_points_input_2 = bsxfun(@plus, chol(S2)'*gpml_randn(0.3, 2, test_points_output_2), m2);  
test_points = [test_points_input_1 test_points_input_2]'; test_points1 = [-ones(1,test_points_output_1) ones(1,test_points_output_2)]';

[a b c d lpt] = gp(hyp, @infEP, meanfunc, covfunc, likfunc, x, y, test_points, ones(n,1));
 figure(9)
 set(gca, 'FontSize', 24)

% training data
 plot(x1(1,:), x1(2,:), 'b+', 'MarkerSize', 10); hold on
plot(x2(1,:), x2(2,:), 'r+', 'MarkerSize', 10)

%testing data
 plot(test_points_input_1(1,:), test_points_input_1(2,:), 'bo', 'MarkerSize', 10); hold on
plot(test_points_input_2(1,:), test_points_input_2(2,:), 'ro', 'MarkerSize', 10)

% lines that were generated after training
 contour(t1, t2, reshape(exp(lp), size(t1)), 0.1:0.1:0.9)
 [c h] = contour(t1, t2, reshape(exp(lp), size(t1)), [0.5 0.5]);
 set(h, 'LineWidth', 2)
 colorbar
 grid
axis([-4 4 -4 4])

% check how many are classified correctly. Class 1 < 0, class 2 > 0
classificationRate = 1 - sum(abs((a > 0) - (test_points1 > 0))) / size(test_points1, 1)

class1 = ( ( (a < 0) - (test_points1 < 0) ) == 0) % where 1 correct, where 0 incorrect
class2 = ( ( (a < 0) - (test_points1 < 0) ) ~= 0)

%a1 = a(class1)
%a2 = a(class2)

a1 = lpt(class1)
a2 = lpt(class2)

for i=1:size(a1)
h1(i)= -(  (exp(a1(i))*log2(exp(a1(i)) )) + ((1-exp(a1(i)))*log2(1-exp(a1(i))))   )

end
for i = 1:size(a2)
   h11(i)=-(  (exp(a2(i))*log2(exp(a2(i)))) + ((1-exp(a2(i)))*log2(1-exp(a2(i))))   ) 
end
h1 = h1'
%h4 = real(h1);

h11 = h11'
%h5 = real(h11)

figure(10)
histogram(h1,10)
figure(11)
histogram(h11,10)
 %[labelspredict,score] = predict(clf,images1(1:1000,:));
% boosting implementation
% clf = fitensemble(images(1:1000,:),labels(1:1000),'AdaBoostM2',100,'Tree')
clf = fitensemble(x,y,'Bag',1000,'Tree','Type','Classification')
[labelspredict,score] =  predict(clf,test_points)
error = sum((labelspredict == test_points1))/size(test_points1,1)

for i = 1:size(score)
score_correct(i) = max(score(i,:))
score_false(i) = min(score(i,:))
end

score_correct = score_correct'
score_false = score_false'
class3 = (labelspredict == test_points1)
a3 = score_correct(class3)
class4 = (labelspredict ~= test_points1)
a4 = score_false(class4)

for i=1:size(a3)
h2(i)= -(  ((a3(i))*log2((a3(i)) )) + ((1-(a3(i)))*log2(1-(a3(i))))   )

end
for i = 1:size(a4)
   h21(i)=-(  ((a4(i))*log2((a4(i)))) + ((1-(a4(i)))*log2(1-(a4(i))))   ) 
end
%h2 = h2'
%h4 = real(h1);

%h21 = h21'


figure(12)
histogram(h2,10)
figure(13)
histogram(h21,10)
if write_fig, print -depsc f8.eps; end
disp(' ')